{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04475597",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed985905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from itertools import product\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6123bb",
   "metadata": {},
   "source": [
    "# 2. Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6686e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model directory: C:/VERGER/Spice_Price_Prediction/Cinnamon/Datasets/Quillings_Dataset\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 12  # 12 months lookback\n",
    "MODEL_DIR = 'cinnamon_models'\n",
    "\n",
    "# Create model directory\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    print(f\"Created model directory: {MODEL_DIR}\")\n",
    "\n",
    "# Initialize preprocessors\n",
    "scaler_features = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "label_encoders = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8b4ba",
   "metadata": {},
   "source": [
    "# 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cae66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_path):\n",
    "    \"\"\"Load and prepare the cinnamon price dataset\"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Rename 'Date' to 'Month' and 'Price' to 'Regional_Price' for consistency\n",
    "    if 'Date' in df.columns:\n",
    "        df = df.rename(columns={'Date': 'Month'})\n",
    "    if 'Price' in df.columns:\n",
    "        df = df.rename(columns={'Price': 'Regional_Price'})\n",
    "\n",
    "    # Convert Month to datetime\n",
    "    df['Month'] = pd.to_datetime(df['Month'])\n",
    "\n",
    "    # Create National_Price as average of all regional prices for each date and grade\n",
    "    national_avg = df.groupby(['Month', 'Grade'])['Regional_Price'].mean().reset_index()\n",
    "    national_avg = national_avg.rename(columns={'Regional_Price': 'National_Price'})\n",
    "    df = df.merge(national_avg, on=['Month', 'Grade'], how='left')\n",
    "\n",
    "    # Create Is_Active_Region (all regions are active in this dataset)\n",
    "    df['Is_Active_Region'] = 1\n",
    "\n",
    "    # Encode categorical variables\n",
    "    for col in ['Grade', 'Region']:\n",
    "        if col not in label_encoders:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "    # Create time-based features\n",
    "    df['Year'] = df['Month'].dt.year\n",
    "    df['Month_num'] = df['Month'].dt.month\n",
    "    df['Quarter'] = df['Month'].dt.quarter\n",
    "\n",
    "    print(\"Creating lag and rolling features...\")\n",
    "\n",
    "    # Sort data\n",
    "    df = df.sort_values(['Grade', 'Region', 'Month'])\n",
    "    \n",
    "    # Create lag features\n",
    "    lag_columns = ['Regional_Price', 'National_Price', 'Temperature', 'Rainfall']\n",
    "    for col in lag_columns:\n",
    "        if col in df.columns:\n",
    "            for lag in [1, 3, 6, 12]:\n",
    "                df[f'{col}_lag_{lag}'] = df.groupby(['Grade', 'Region'])[col].shift(lag)\n",
    "\n",
    "    # Create rolling averages\n",
    "    for col in ['Regional_Price', 'Temperature', 'Rainfall']:\n",
    "        if col in df.columns:\n",
    "            for window in [3, 6, 12]:\n",
    "                df[f'{col}_rolling_{window}'] = df.groupby(['Grade', 'Region'])[col].transform(\n",
    "                    lambda x: x.rolling(window).mean()\n",
    "                )\n",
    "\n",
    "    print(f\"Final data shape after feature engineering: {df.shape}\")\n",
    "    print(f\"Unique grades: {df['Grade'].unique()}\")\n",
    "    print(f\"Unique regions: {df['Region'].unique()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6165e",
   "metadata": {},
   "source": [
    "# 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5307e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_distribution_by_grade(df):\n",
    "    \"\"\"Plot price distribution by grade\"\"\"\n",
    "    df_clean = df.dropna(subset=['Regional_Price'])\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(data=df_clean, x='Grade', y='Regional_Price', ax=ax1)\n",
    "    ax1.set_title('Price Distribution by Grade (Box Plot)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Grade', fontsize=12)\n",
    "    ax1.set_ylabel('Regional Price', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df_clean, x='Grade', y='Regional_Price', ax=ax2)\n",
    "    ax2.set_title('Price Distribution by Grade (Violin Plot)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Grade', fontsize=12)\n",
    "    ax2.set_ylabel('Regional Price', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Mean prices with error bars\n",
    "    grade_stats = df_clean.groupby('Grade')['Regional_Price'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    grade_stats['se'] = grade_stats['std'] / np.sqrt(grade_stats['count'])\n",
    "    \n",
    "    ax3.bar(grade_stats['Grade'], grade_stats['mean'], \n",
    "            yerr=grade_stats['se'], capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    ax3.set_title('Average Price by Grade (with Standard Error)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Grade', fontsize=12)\n",
    "    ax3.set_ylabel('Average Regional Price', fontsize=12)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, (grade, mean_price) in enumerate(zip(grade_stats['Grade'], grade_stats['mean'])):\n",
    "        ax3.text(i, mean_price + grade_stats['se'].iloc[i], f'{mean_price:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Histogram\n",
    "    for grade in df_clean['Grade'].unique():\n",
    "        grade_data = df_clean[df_clean['Grade'] == grade]['Regional_Price']\n",
    "        ax4.hist(grade_data, alpha=0.6, label=grade, bins=20, density=True)\n",
    "    \n",
    "    ax4.set_title('Price Distribution Histograms by Grade', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Regional Price', fontsize=12)\n",
    "    ax4.set_ylabel('Density', fontsize=12)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPrice Distribution Summary by Grade:\")\n",
    "    print(\"=\" * 50)\n",
    "    summary_stats = df_clean.groupby('Grade')['Regional_Price'].describe()\n",
    "    print(summary_stats.round(2))\n",
    "\n",
    "\n",
    "def plot_feature_correlation_matrix(df):\n",
    "    \"\"\"Plot feature correlation matrix\"\"\"\n",
    "    numeric_features = [\n",
    "        'Regional_Price', 'National_Price', 'Seasonal_Impact',\n",
    "        'Local_Production_Volume', 'Local_Export_Volume',\n",
    "        'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "        'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate',\n",
    "        'Fuel_Price', 'Year', 'Month_num', 'Quarter', 'Grade_encoded',\n",
    "        'Region_encoded', 'Is_Active_Region'\n",
    "    ]\n",
    "    \n",
    "    available_features = [col for col in numeric_features if col in df.columns]\n",
    "    df_numeric = df[available_features].select_dtypes(include=[np.number])\n",
    "    correlation_matrix = df_numeric.corr()\n",
    "    \n",
    "    plt.figure(figsize=(16, 14))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                annot_kws={'size': 8})\n",
    "    \n",
    "    plt.title('Feature Correlation Matrix\\n(Cinnamon Price Forecasting Dataset)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    print(\"=\" * 60)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        for feature1, feature2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"{feature1} â†” {feature2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"No feature pairs with |correlation| > 0.7 found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f8933",
   "metadata": {},
   "source": [
    "# 5. Sequence Peparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ba2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(df, sequence_length=12, target_col='Regional_Price'):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    feature_cols = [\n",
    "        'Grade_encoded', 'Region_encoded', 'Is_Active_Region',\n",
    "        'National_Price', 'Seasonal_Impact', 'Local_Production_Volume',\n",
    "        'Local_Export_Volume', 'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "        'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate', 'Fuel_Price',\n",
    "        'Year', 'Month_num', 'Quarter'\n",
    "    ]\n",
    "\n",
    "    # Add lag and rolling features\n",
    "    lag_cols = [col for col in df.columns if 'lag_' in col or 'rolling_' in col]\n",
    "    feature_cols.extend(lag_cols)\n",
    "\n",
    "    # Fill NaN values\n",
    "    df_clean = df.copy()\n",
    "    df_clean = df_clean.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    X_sequences, y_sequences, metadata = [], [], []\n",
    "\n",
    "    for grade in df_clean['Grade'].unique():\n",
    "        for region in df_clean['Region'].unique():\n",
    "            subset = df_clean[(df_clean['Grade'] == grade) & (df_clean['Region'] == region)].sort_values('Month')\n",
    "\n",
    "            if len(subset) < sequence_length + 1:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(subset) - sequence_length):\n",
    "                X_seq = subset.iloc[i:i + sequence_length][feature_cols].values\n",
    "                y_seq = subset.iloc[i + sequence_length][target_col]\n",
    "\n",
    "                X_sequences.append(X_seq)\n",
    "                y_sequences.append(y_seq)\n",
    "                metadata.append({\n",
    "                    'grade': grade,\n",
    "                    'region': region,\n",
    "                    'date': subset.iloc[i + sequence_length]['Month']\n",
    "                })\n",
    "\n",
    "    print(f\"Total sequences created: {len(X_sequences)}\")\n",
    "    return np.array(X_sequences), np.array(y_sequences), metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cfb29",
   "metadata": {},
   "source": [
    "# 6. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdba929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model_tunable(units1=128, units2=64, dropout1=0.2, dropout2=0.2, \n",
    "                            dense_units=32, optimizer='adam', learning_rate=0.001, \n",
    "                            layer_type='LSTM', use_batch_norm=False, input_shape=None):\n",
    "    \"\"\"Build tunable LSTM model\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First RNN layer\n",
    "    if layer_type == 'LSTM':\n",
    "        model.add(LSTM(units1, return_sequences=True, input_shape=input_shape))\n",
    "    elif layer_type == 'GRU':\n",
    "        model.add(GRU(units1, return_sequences=True, input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(SimpleRNN(units1, return_sequences=True, input_shape=input_shape))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout1))\n",
    "    \n",
    "    # Second RNN layer\n",
    "    if layer_type == 'LSTM':\n",
    "        model.add(LSTM(units2, return_sequences=False))\n",
    "    elif layer_type == 'GRU':\n",
    "        model.add(GRU(units2, return_sequences=False))\n",
    "    else:\n",
    "        model.add(SimpleRNN(units2, return_sequences=False))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout2))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Configure optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccbbb5",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebf3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"Hyperparameter tuning class\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, input_shape):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.input_shape = input_shape\n",
    "        self.best_params = None\n",
    "        self.best_score = float('inf')\n",
    "        self.tuning_results = []\n",
    "    \n",
    "    def optuna_tuning(self, n_trials=50):\n",
    "        \"\"\"Optuna-based hyperparameter tuning\"\"\"\n",
    "        print(\"\\nðŸŽ¯ Starting Optuna Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'units1': trial.suggest_categorical('units1', [64, 128, 256, 512]),\n",
    "                'units2': trial.suggest_categorical('units2', [32, 64, 128, 256]),\n",
    "                'dropout1': trial.suggest_float('dropout1', 0.1, 0.5, step=0.1),\n",
    "                'dropout2': trial.suggest_float('dropout2', 0.1, 0.5, step=0.1),\n",
    "                'dense_units': trial.suggest_categorical('dense_units', [16, 32, 64, 128]),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n",
    "                'layer_type': trial.suggest_categorical('layer_type', ['LSTM', 'GRU']),\n",
    "                'use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),\n",
    "                'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                model = build_lstm_model_tunable(**params, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "                return val_loss\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Trial failed: {e}\")\n",
    "                return float('inf')\n",
    "        \n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Optuna Tuning Complete!\")\n",
    "        print(f\"Best validation loss: {study.best_value:.6f}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        \n",
    "        return study.best_params, study.best_value\n",
    "    \n",
    "    def random_search_tuning(self, n_trials=30):\n",
    "        \"\"\"Random search hyperparameter tuning\"\"\"\n",
    "        print(\"\\nðŸŽ² Starting Random Search Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for i in range(n_trials):\n",
    "            params = {\n",
    "                'units1': np.random.choice([64, 128, 256, 512]),\n",
    "                'units2': np.random.choice([32, 64, 128, 256]),\n",
    "                'dropout1': np.random.uniform(0.1, 0.5),\n",
    "                'dropout2': np.random.uniform(0.1, 0.5),\n",
    "                'dense_units': np.random.choice([16, 32, 64, 128]),\n",
    "                'learning_rate': 10 ** np.random.uniform(-4, -2),\n",
    "                'layer_type': np.random.choice(['LSTM', 'GRU']),\n",
    "                'use_batch_norm': np.random.choice([True, False]),\n",
    "                'optimizer': np.random.choice(['adam', 'rmsprop'])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nTrial {i+1}/{n_trials}\")\n",
    "                \n",
    "                model = build_lstm_model_tunable(**params, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                result = {\n",
    "                    'trial': i+1,\n",
    "                    'params': params.copy(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_mae': min(history.history['val_mae'])\n",
    "                }\n",
    "                \n",
    "                self.tuning_results.append(result)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = params.copy()\n",
    "                    print(f\"âœ… New best validation loss: {val_loss:.6f}\")\n",
    "                else:\n",
    "                    print(f\"   Validation loss: {val_loss:.6f}\")\n",
    "                \n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Trial {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_params = best_params\n",
    "        self.best_score = best_val_loss\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Random Search Complete!\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        return best_params, best_val_loss\n",
    "\n",
    "\n",
    "def perform_hyperparameter_tuning(X_train, y_train, X_val, y_val, input_shape, \n",
    "                                 method='optuna', n_trials=30):\n",
    "    \"\"\"Main function to perform hyperparameter tuning\"\"\"\n",
    "    print(f\"\\nðŸš€ Starting Hyperparameter Tuning using {method.upper()} method...\")\n",
    "    \n",
    "    tuner = HyperparameterTuner(X_train, y_train, X_val, y_val, input_shape)\n",
    "    \n",
    "    if method == 'optuna':\n",
    "        best_params, best_score = tuner.optuna_tuning(n_trials=n_trials)\n",
    "    elif method == 'random':\n",
    "        best_params, best_score = tuner.random_search_tuning(n_trials=n_trials)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'optuna' or 'random'\")\n",
    "    \n",
    "    return best_params, best_score, tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5c363",
   "metadata": {},
   "source": [
    "# 8. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f96a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, best_params=None):\n",
    "    \"\"\"Build LSTM model with optional best parameters\"\"\"\n",
    "    if best_params is None:\n",
    "        best_params = {\n",
    "            'units1': 128,\n",
    "            'units2': 64,\n",
    "            'dropout1': 0.2,\n",
    "            'dropout2': 0.2,\n",
    "            'dense_units': 32,\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'layer_type': 'LSTM',\n",
    "            'use_batch_norm': False\n",
    "        }\n",
    "    \n",
    "    return build_lstm_model_tunable(**best_params, input_shape=input_shape)\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training vs validation loss\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "    ax1.set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(history.history['mae'], label='Training MAE', linewidth=2, color='blue')\n",
    "    ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2, color='red')\n",
    "    ax2.set_title('Model MAE Over Epochs', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('MAE', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_mae = history.history['mae'][-1]\n",
    "    final_val_mae = history.history['val_mae'][-1]\n",
    "    \n",
    "    print(f\"\\nFinal Training Metrics:\")\n",
    "    print(f\"Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Training MAE: {final_train_mae:.4f}\")\n",
    "    print(f\"Validation MAE: {final_val_mae:.4f}\")\n",
    "\n",
    "\n",
    "def train_model(df, use_tuning=True, tuning_method='optuna', n_tuning_trials=20):\n",
    "    \"\"\"Train the forecasting model with optional hyperparameter tuning\"\"\"\n",
    "    global scaler_features, scaler_target\n",
    "    \n",
    "    print(\"Preparing sequences...\")\n",
    "    X, y, metadata = prepare_sequences(df, SEQUENCE_LENGTH)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No sequences could be created. Check if there's enough data.\")\n",
    "\n",
    "    print(f\"Created {len(X)} sequences with shape {X.shape}\")\n",
    "\n",
    "    # Scale features and target\n",
    "    print(\"Scaling features...\")\n",
    "    n_samples, n_timesteps, n_features = X.shape\n",
    "    X_reshaped = X.reshape(-1, n_features)\n",
    "    X_scaled_reshaped = scaler_features.fit_transform(X_reshaped)\n",
    "    X_scaled = X_scaled_reshaped.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "    y_scaled = scaler_target.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Train-validation-test split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Validation set shape: X={X_val.shape}, y={y_val.shape}\")\n",
    "    print(f\"Test set shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    best_params = None\n",
    "    tuner = None\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    if use_tuning:\n",
    "        print(f\"\\nðŸ”§ Performing hyperparameter tuning using {tuning_method} method...\")\n",
    "        best_params, best_score, tuner = perform_hyperparameter_tuning(\n",
    "            X_train, y_train, X_val, y_val, input_shape, \n",
    "            method=tuning_method, n_trials=n_tuning_trials\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Hyperparameter Tuning Results:\")\n",
    "        print(f\"Best validation loss: {best_score:.6f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping hyperparameter tuning, using default parameters...\")\n",
    "\n",
    "    # Build and train final model\n",
    "    print(\"\\nBuilding final model with optimized parameters...\")\n",
    "    model = build_lstm_model(input_shape, best_params)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Final Model Architecture:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nTraining final model...\")\n",
    "    final_epochs = 150 if use_tuning else 100\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=final_epochs,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=15, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(patience=8, factor=0.5, verbose=1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_unscaled = scaler_target.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_unscaled = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))\n",
    "    r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Final Model Performance on Test Set:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'mae': mae, \n",
    "        'rmse': rmse, \n",
    "        'r2': r2,\n",
    "        'best_params': best_params,\n",
    "        'tuning_method': tuning_method if use_tuning else None,\n",
    "        'tuning_used': use_tuning,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'final_train_loss': history.history['loss'][-1],\n",
    "        'final_val_loss': history.history['val_loss'][-1]\n",
    "    }\n",
    "    \n",
    "    if tuner and hasattr(tuner, 'tuning_results'):\n",
    "        results['tuning_results'] = tuner.tuning_results\n",
    "    \n",
    "    return model, history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb719a",
   "metadata": {},
   "source": [
    "# 9. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced252c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, metrics, model_dir=MODEL_DIR):\n",
    "    \"\"\"Save the trained model and preprocessors\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_save_dir = os.path.join(model_dir, f\"cinnamon_model_{timestamp}\")\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Saving model to: {model_save_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Save Keras model\n",
    "        model_path = os.path.join(model_save_dir, \"lstm_model.keras\")\n",
    "        model.save(model_path)\n",
    "        print(f\"âœ… Keras model saved: lstm_model.keras\")\n",
    "        \n",
    "        # Save scalers\n",
    "        scalers_path = os.path.join(model_save_dir, \"scalers.pkl\")\n",
    "        scalers = {\n",
    "            'scaler_features': scaler_features,\n",
    "            'scaler_target': scaler_target\n",
    "        }\n",
    "        with open(scalers_path, 'wb') as f:\n",
    "            pickle.dump(scalers, f)\n",
    "        print(f\"âœ… Scalers saved: scalers.pkl\")\n",
    "        \n",
    "        # Save label encoders\n",
    "        encoders_path = os.path.join(model_save_dir, \"label_encoders.pkl\")\n",
    "        with open(encoders_path, 'wb') as f:\n",
    "            pickle.dump(label_encoders, f)\n",
    "        print(f\"âœ… Label encoders saved: label_encoders.pkl\")\n",
    "        \n",
    "        # Save model configuration\n",
    "        config = {\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "            'model_architecture': {\n",
    "                'input_shape': model.input_shape,\n",
    "                'layers': [str(layer.__class__.__name__) for layer in model.layers],\n",
    "                'total_params': model.count_params()\n",
    "            },\n",
    "            'training_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'mae': float(metrics['mae']),\n",
    "                'rmse': float(metrics['rmse']),\n",
    "                'r2': float(metrics['r2']),\n",
    "                'tuning_used': metrics.get('tuning_used', False),\n",
    "                'tuning_method': metrics.get('tuning_method', None),\n",
    "                'best_params': metrics.get('best_params', None),\n",
    "                'epochs_trained': metrics.get('epochs_trained', 0),\n",
    "                'final_train_loss': float(metrics.get('final_train_loss', 0)),\n",
    "                'final_val_loss': float(metrics.get('final_val_loss', 0))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if 'tuning_results' in metrics and metrics['tuning_results']:\n",
    "            tuning_results_path = os.path.join(model_save_dir, \"tuning_results.json\")\n",
    "            with open(tuning_results_path, 'w') as f:\n",
    "                json.dump(metrics['tuning_results'], f, indent=2, default=str)\n",
    "            print(f\"âœ… Hyperparameter tuning results saved: tuning_results.json\")\n",
    "        \n",
    "        config_path = os.path.join(model_save_dir, \"model_config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2, default=str)\n",
    "        print(f\"âœ… Model configuration saved: model_config.json\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Model successfully saved to: {model_save_dir}\")\n",
    "        print(f\"\\nðŸ“Š Saved Model Summary:\")\n",
    "        print(f\"  â€¢ Performance: MAE={metrics['mae']:.2f}, RMSE={metrics['rmse']:.2f}, RÂ²={metrics['r2']:.4f}\")\n",
    "        if metrics.get('tuning_used'):\n",
    "            print(f\"  â€¢ Hyperparameter tuning: {metrics['tuning_method']} method used\")\n",
    "            print(f\"  â€¢ Best parameters found and applied\")\n",
    "        print(f\"  â€¢ Training epochs: {metrics.get('epochs_trained', 'N/A')}\")\n",
    "        print(f\"  â€¢ Total parameters: {model.count_params():,}\")\n",
    "        \n",
    "        return model_save_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_saved_model(model_path):\n",
    "    \"\"\"Load a previously saved model\"\"\"\n",
    "    global scaler_features, scaler_target, label_encoders\n",
    "    \n",
    "    print(f\"ðŸ“‚ Loading model from: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load Keras model\n",
    "        keras_model_path = os.path.join(model_path, \"lstm_model.keras\")\n",
    "        model = load_model(keras_model_path)\n",
    "        print(f\"âœ… Keras model loaded\")\n",
    "        \n",
    "        # Load scalers\n",
    "        scalers_path = os.path.join(model_path, \"scalers.pkl\")\n",
    "        with open(scalers_path, 'rb') as f:\n",
    "            scalers = pickle.load(f)\n",
    "        scaler_features = scalers['scaler_features']\n",
    "        scaler_target = scalers['scaler_target']\n",
    "        print(f\"âœ… Scalers loaded\")\n",
    "        \n",
    "        # Load label encoders\n",
    "        encoders_path = os.path.join(model_path, \"label_encoders.pkl\")\n",
    "        with open(encoders_path, 'rb') as f:\n",
    "            label_encoders = pickle.load(f)\n",
    "        print(f\"âœ… Label encoders loaded\")\n",
    "        \n",
    "        # Load configuration\n",
    "        config_path = os.path.join(model_path, \"model_config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        print(f\"ðŸŽ‰ Model successfully loaded!\")\n",
    "        print(f\"ðŸ“Š Performance: MAE={config['training_info']['mae']:.2f}, \"\n",
    "              f\"RMSE={config['training_info']['rmse']:.2f}, \"\n",
    "              f\"RÂ²={config['training_info']['r2']:.4f}\")\n",
    "        \n",
    "        if config['training_info'].get('tuning_used'):\n",
    "            print(f\"ðŸ”§ This model was trained with {config['training_info']['tuning_method']} hyperparameter tuning\")\n",
    "        \n",
    "        return model, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1e766",
   "metadata": {},
   "source": [
    "# 10. Forecasting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4e5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FEATURE_COLS = [\n",
    "    'Grade_encoded', 'Region_encoded', 'Is_Active_Region',\n",
    "    'National_Price', 'Seasonal_Impact', 'Local_Production_Volume',\n",
    "    'Local_Export_Volume', 'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "    'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate', 'Fuel_Price',\n",
    "    'Year', 'Month_num', 'Quarter',\n",
    "    'Regional_Price_lag_1', 'Regional_Price_lag_3', 'Regional_Price_lag_6', 'Regional_Price_lag_12',\n",
    "    'National_Price_lag_1', 'National_Price_lag_3', 'National_Price_lag_6', 'National_Price_lag_12',\n",
    "    'Temperature_lag_1', 'Temperature_lag_3', 'Temperature_lag_6', 'Temperature_lag_12',\n",
    "    'Rainfall_lag_1', 'Rainfall_lag_3', 'Rainfall_lag_6', 'Rainfall_lag_12',\n",
    "    'Regional_Price_rolling_3', 'Regional_Price_rolling_6', 'Regional_Price_rolling_12',\n",
    "    'Temperature_rolling_3', 'Temperature_rolling_6', 'Temperature_rolling_12',\n",
    "    'Rainfall_rolling_3', 'Rainfall_rolling_6', 'Rainfall_rolling_12'\n",
    "]\n",
    "\n",
    "\n",
    "def forecast_prices(model, df, grade, region, months_ahead=12):\n",
    "    \"\"\"Generate price forecasts\"\"\"\n",
    "    subset = df[(df['Grade'] == grade) & (df['Region'] == region)].sort_values('Month')\n",
    "    last_row = subset.iloc[-1]\n",
    "    last_date = last_row['Month']\n",
    "\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1),\n",
    "                                 periods=months_ahead, freq='MS')\n",
    "    \n",
    "    # Generate future rows\n",
    "    future_rows = []\n",
    "    for future_date in future_dates:\n",
    "        row = last_row.copy()\n",
    "        row['Month'] = future_date\n",
    "        row['Year'] = future_date.year\n",
    "        row['Month_num'] = future_date.month\n",
    "        row['Quarter'] = future_date.quarter\n",
    "        row['Temperature'] = last_row['Temperature'] + 2 * np.sin(2*np.pi*(future_date.month-1)/12) + np.random.normal(0,0.5)\n",
    "        row['Rainfall'] = max(0, last_row['Rainfall'] + 20 * np.sin(2*np.pi*(future_date.month-1)/12) + np.random.normal(0,10))\n",
    "        row['Exchange_Rate'] = last_row['Exchange_Rate'] * (1 + np.random.normal(0.001,0.005))\n",
    "        row['Inflation_Rate'] = last_row['Inflation_Rate'] + np.random.normal(0,0.1)\n",
    "        row['Fuel_Price'] = last_row['Fuel_Price'] * (1 + np.random.normal(0.002,0.02))\n",
    "        future_rows.append(row)\n",
    "\n",
    "    future_df = pd.DataFrame(future_rows)\n",
    "    extended_df = pd.concat([subset, future_df], ignore_index=True).sort_values('Month')\n",
    "\n",
    "    # Recreate lag and rolling features\n",
    "    for col in ['Regional_Price','National_Price','Temperature','Rainfall']:\n",
    "        for lag in [1,3,6,12]:\n",
    "            extended_df[f'{col}_lag_{lag}'] = extended_df.groupby(['Grade','Region'])[col].shift(lag)\n",
    "        for window in [3,6,12]:\n",
    "            extended_df[f'{col}_rolling_{window}'] = extended_df.groupby(['Grade','Region'])[col].transform(lambda x: x.rolling(window).mean())\n",
    "\n",
    "    feature_cols = [c for c in TRAIN_FEATURE_COLS if c in extended_df.columns]\n",
    "\n",
    "    forecasts = []\n",
    "    historical_data = extended_df[extended_df['Month'] <= last_date]\n",
    "\n",
    "    for i in range(months_ahead):\n",
    "        current_data = extended_df.iloc[len(historical_data)-SEQUENCE_LENGTH+i : len(historical_data)+i]\n",
    "        if len(current_data) < SEQUENCE_LENGTH:\n",
    "            padding_needed = SEQUENCE_LENGTH - len(current_data)\n",
    "            last_known = historical_data.iloc[-1:].copy()\n",
    "            padding_data = pd.concat([last_known]*padding_needed, ignore_index=True)\n",
    "            current_data = pd.concat([padding_data, current_data], ignore_index=True).iloc[-SEQUENCE_LENGTH:]\n",
    "\n",
    "        sequence = current_data[feature_cols].ffill().bfill().values\n",
    "        sequence_flat = sequence.reshape(-1, sequence.shape[-1])\n",
    "        sequence_scaled_flat = scaler_features.transform(sequence_flat)\n",
    "        sequence_scaled = sequence_scaled_flat.reshape(sequence.shape)\n",
    "\n",
    "        next_pred = model.predict(sequence_scaled.reshape(1, SEQUENCE_LENGTH, -1), verbose=0)\n",
    "        next_pred_unscaled = scaler_target.inverse_transform(next_pred)[0][0]\n",
    "        forecasts.append(next_pred_unscaled)\n",
    "\n",
    "        future_idx = len(historical_data)+i\n",
    "        extended_df.iloc[future_idx, extended_df.columns.get_loc('Regional_Price')] = next_pred_unscaled\n",
    "        extended_df.iloc[future_idx, extended_df.columns.get_loc('National_Price')] = next_pred_unscaled\n",
    "\n",
    "    return forecasts, future_dates\n",
    "\n",
    "\n",
    "def plot_forecast_results(df, model, grade, region, months_ahead=12):\n",
    "    \"\"\"Plot historical data with forecast results\"\"\"\n",
    "    try:\n",
    "        subset = df[(df['Grade'] == grade) & (df['Region'] == region)].sort_values('Month')\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            print(f\"No data found for {grade} in {region}\")\n",
    "            return None, None\n",
    "        \n",
    "        forecasts, future_dates = forecast_prices(model, df, grade, region, months_ahead)\n",
    "        \n",
    "        plt.figure(figsize=(16, 8))\n",
    "        \n",
    "        # Plot historical data\n",
    "        plt.plot(subset['Month'], subset['Regional_Price'], \n",
    "                label='Historical Prices', linewidth=2, color='blue', marker='o', markersize=4)\n",
    "        \n",
    "        # Bridge connection\n",
    "        last_historical_date = subset['Month'].iloc[-1]\n",
    "        last_historical_price = subset['Regional_Price'].iloc[-1]\n",
    "        first_forecast_date = future_dates[0]\n",
    "        first_forecast_price = forecasts[0]\n",
    "        \n",
    "        plt.plot([last_historical_date, first_forecast_date], \n",
    "                [last_historical_price, first_forecast_price], \n",
    "                color='orange', linewidth=2, linestyle='-', alpha=0.8, \n",
    "                label='Historical-Forecast Bridge')\n",
    "        \n",
    "        # Plot forecasts\n",
    "        extended_forecast_dates = [last_historical_date] + list(future_dates)\n",
    "        extended_forecasts = [last_historical_price] + list(forecasts)\n",
    "        \n",
    "        plt.plot(extended_forecast_dates, extended_forecasts, \n",
    "                label='Forecasted Prices', linewidth=2, color='red', \n",
    "                marker='s', markersize=5, linestyle='--', alpha=0.9)\n",
    "        \n",
    "        plt.axvline(x=last_historical_date, color='blue', linestyle=':', alpha=0.5, linewidth=1, \n",
    "                   label='Forecast Start')\n",
    "        \n",
    "        plt.title(f'Cinnamon Price Forecast: {grade.title()} Grade in {region.title()}\\n'\n",
    "                 f'Historical Data vs {months_ahead}-Month Forecast', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Regional Price', fontsize=12)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Confidence bands\n",
    "        if len(forecasts) > 1:\n",
    "            forecast_std = np.std(subset['Regional_Price'].tail(12))\n",
    "            upper_bound = np.array(extended_forecasts[1:]) + 1.96 * forecast_std\n",
    "            lower_bound = np.array(extended_forecasts[1:]) - 1.96 * forecast_std\n",
    "            \n",
    "            plt.fill_between(future_dates, lower_bound, upper_bound, \n",
    "                           alpha=0.2, color='red', label='95% Confidence Interval')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print forecast summary\n",
    "        print(f\"\\n{grade.title()} Grade Forecast for {region.title()}:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, (date, price) in enumerate(zip(future_dates, forecasts), 1):\n",
    "            print(f\"Month {i:2d} ({date.strftime('%Y-%m')}): LKR {price:8.2f}\")\n",
    "        \n",
    "        print(f\"\\nForecast Statistics:\")\n",
    "        print(f\"Average Forecast Price: LKR {np.mean(forecasts):.2f}\")\n",
    "        print(f\"Price Range: LKR {np.min(forecasts):.2f} - LKR {np.max(forecasts):.2f}\")\n",
    "        \n",
    "        if len(forecasts) > 1:\n",
    "            trend = (forecasts[-1] - forecasts[0]) / len(forecasts)\n",
    "            trend_direction = \"increasing\" if trend > 0 else \"decreasing\"\n",
    "            print(f\"Overall Trend: {trend_direction} by LKR {abs(trend):.2f} per month\")\n",
    "        \n",
    "        return forecasts, future_dates\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting forecast results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc8da9",
   "metadata": {},
   "source": [
    "# 11. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911ea5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ CINNAMON PRICE FORECASTING WITH LSTM\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Step 1: Loading and preparing data...\n",
      "Loading data from your_dataset.csv...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Step 1: Load and prepare data\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Step 1: Loading and preparing data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df = \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Data loaded successfully! Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Step 2: Visualizations\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m(data_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load and prepare the cinnamon price dataset\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitial data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.columns.tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'your_dataset.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸš€ CINNAMON PRICE FORECASTING WITH LSTM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # CONFIGURATION - MODIFY THESE PARAMETERS\n",
    "    DATA_PATH = 'your_dataset.csv'  # âš ï¸ CHANGE THIS TO YOUR CSV FILE PATH\n",
    "    USE_HYPERPARAMETER_TUNING = True  # Set to False to skip tuning\n",
    "    TUNING_METHOD = 'optuna'  # Options: 'optuna', 'random'\n",
    "    N_TUNING_TRIALS = 50  # Number of trials for tuning\n",
    "    \n",
    "    # Step 1: Load and prepare data\n",
    "    print(\"\\nðŸ“Š Step 1: Loading and preparing data...\")\n",
    "    df = load_and_prepare_data(DATA_PATH)\n",
    "    print(f\"âœ… Data loaded successfully! Shape: {df.shape}\")\n",
    "    \n",
    "    # Step 2: Visualizations\n",
    "    print(\"\\nðŸ“ˆ Step 2: Generating visualizations...\")\n",
    "    plot_price_distribution_by_grade(df)\n",
    "    plot_feature_correlation_matrix(df)\n",
    "    \n",
    "    # Step 3: Train model\n",
    "    print(\"\\nðŸ¤– Step 3: Training LSTM model...\")\n",
    "    model, history, metrics = train_model(\n",
    "        df, \n",
    "        use_tuning=USE_HYPERPARAMETER_TUNING,\n",
    "        tuning_method=TUNING_METHOD,\n",
    "        n_tuning_trials=N_TUNING_TRIALS\n",
    "    )\n",
    "    \n",
    "    # Step 4: Save model\n",
    "    print(\"\\nðŸ’¾ Step 4: Saving trained model...\")\n",
    "    saved_model_path = save_model(model, metrics)\n",
    "    print(f\"âœ… Model saved at: {saved_model_path}\")\n",
    "    \n",
    "    # Step 5: Generate forecasts\n",
    "    print(\"\\nðŸ”® Step 5: Generating sample forecasts...\")\n",
    "    available_grades = df['Grade'].unique()\n",
    "    available_regions = df['Region'].unique()\n",
    "    \n",
    "    print(f\"Available grades: {available_grades}\")\n",
    "    print(f\"Available regions: {available_regions}\")\n",
    "    \n",
    "    # Generate forecast for first combination\n",
    "    if len(available_grades) > 0 and len(available_regions) > 0:\n",
    "        grade_to_forecast = available_grades[0]\n",
    "        region_to_forecast = available_regions[0]\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Generating 6-month forecast for {grade_to_forecast} in {region_to_forecast}...\")\n",
    "        forecasts, future_dates = plot_forecast_results(\n",
    "            df, model, grade_to_forecast, region_to_forecast, months_ahead=6\n",
    "        )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸŽ¯ TRAINING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if metrics.get('tuning_used'):\n",
    "        print(f\"âœ… Hyperparameter tuning: {metrics['tuning_method']} method used\")\n",
    "        print(f\"ðŸ† Best parameters found:\")\n",
    "        for key, value in metrics['best_params'].items():\n",
    "            print(f\"   â€¢ {key}: {value}\")\n",
    "    else:\n",
    "        print(\"âŒ Hyperparameter tuning was skipped - default parameters used\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Final Model Performance:\")\n",
    "    print(f\"   â€¢ MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"   â€¢ RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"   â€¢ RÂ²: {metrics['r2']:.4f}\")\n",
    "    print(f\"   â€¢ Training epochs: {metrics['epochs_trained']}\")\n",
    "    \n",
    "    print(\"\\nâœ… Training complete! Model is ready for forecasting.\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
