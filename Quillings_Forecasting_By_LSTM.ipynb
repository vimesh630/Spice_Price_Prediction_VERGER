{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04475597",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed985905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from itertools import product\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6123bb",
   "metadata": {},
   "source": [
    "# 2. Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6686e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model directory: C:/VERGER/Spice_Price_Prediction/Cinnamon/Datasets/Quillings_Dataset\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 12  # 12 months lookback\n",
    "MODEL_DIR = 'C:/VERGER/Spice_Price_Prediction/Cinnamon/Datasets/Quillings_Dataset'\n",
    "\n",
    "# Create model directory\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    print(f\"Created model directory: {MODEL_DIR}\")\n",
    "\n",
    "# Initialize preprocessors\n",
    "scaler_features = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "label_encoders = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8b4ba",
   "metadata": {},
   "source": [
    "# 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cae66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_path):\n",
    "    \"\"\"Load and prepare the cinnamon price dataset\"\"\"\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Rename 'Date' to 'Month' and 'Price' to 'Regional_Price' for consistency\n",
    "    if 'Date' in df.columns:\n",
    "        df = df.rename(columns={'Date': 'Month'})\n",
    "    if 'Price' in df.columns:\n",
    "        df = df.rename(columns={'Price': 'Regional_Price'})\n",
    "\n",
    "    # Convert Month to datetime\n",
    "    df['Month'] = pd.to_datetime(df['Month'])\n",
    "\n",
    "    # Create National_Price as average of all regional prices for each date and grade\n",
    "    national_avg = df.groupby(['Month', 'Grade'])['Regional_Price'].mean().reset_index()\n",
    "    national_avg = national_avg.rename(columns={'Regional_Price': 'National_Price'})\n",
    "    df = df.merge(national_avg, on=['Month', 'Grade'], how='left')\n",
    "\n",
    "    # Create Is_Active_Region (all regions are active in this dataset)\n",
    "    df['Is_Active_Region'] = 1\n",
    "\n",
    "    # Encode categorical variables\n",
    "    for col in ['Grade', 'Region']:\n",
    "        if col not in label_encoders:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "    # Create time-based features\n",
    "    df['Year'] = df['Month'].dt.year\n",
    "    df['Month_num'] = df['Month'].dt.month\n",
    "    df['Quarter'] = df['Month'].dt.quarter\n",
    "\n",
    "    print(\"Creating lag and rolling features...\")\n",
    "\n",
    "    # Sort data\n",
    "    df = df.sort_values(['Grade', 'Region', 'Month'])\n",
    "    \n",
    "    # Create lag features\n",
    "    lag_columns = ['Regional_Price', 'National_Price', 'Temperature', 'Rainfall']\n",
    "    for col in lag_columns:\n",
    "        if col in df.columns:\n",
    "            for lag in [1, 3, 6, 12]:\n",
    "                df[f'{col}_lag_{lag}'] = df.groupby(['Grade', 'Region'])[col].shift(lag)\n",
    "\n",
    "    # Create rolling averages\n",
    "    for col in ['Regional_Price', 'Temperature', 'Rainfall']:\n",
    "        if col in df.columns:\n",
    "            for window in [3, 6, 12]:\n",
    "                df[f'{col}_rolling_{window}'] = df.groupby(['Grade', 'Region'])[col].transform(\n",
    "                    lambda x: x.rolling(window).mean()\n",
    "                )\n",
    "\n",
    "    print(f\"Final data shape after feature engineering: {df.shape}\")\n",
    "    print(f\"Unique grades: {df['Grade'].unique()}\")\n",
    "    print(f\"Unique regions: {df['Region'].unique()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6165e",
   "metadata": {},
   "source": [
    "# 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5307e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_distribution_by_grade(df):\n",
    "    \"\"\"Plot price distribution by grade\"\"\"\n",
    "    df_clean = df.dropna(subset=['Regional_Price'])\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(data=df_clean, x='Grade', y='Regional_Price', ax=ax1)\n",
    "    ax1.set_title('Price Distribution by Grade (Box Plot)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Grade', fontsize=12)\n",
    "    ax1.set_ylabel('Regional Price', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df_clean, x='Grade', y='Regional_Price', ax=ax2)\n",
    "    ax2.set_title('Price Distribution by Grade (Violin Plot)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Grade', fontsize=12)\n",
    "    ax2.set_ylabel('Regional Price', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Mean prices with error bars\n",
    "    grade_stats = df_clean.groupby('Grade')['Regional_Price'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    grade_stats['se'] = grade_stats['std'] / np.sqrt(grade_stats['count'])\n",
    "    \n",
    "    ax3.bar(grade_stats['Grade'], grade_stats['mean'], \n",
    "            yerr=grade_stats['se'], capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    ax3.set_title('Average Price by Grade (with Standard Error)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Grade', fontsize=12)\n",
    "    ax3.set_ylabel('Average Regional Price', fontsize=12)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, (grade, mean_price) in enumerate(zip(grade_stats['Grade'], grade_stats['mean'])):\n",
    "        ax3.text(i, mean_price + grade_stats['se'].iloc[i], f'{mean_price:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Histogram\n",
    "    for grade in df_clean['Grade'].unique():\n",
    "        grade_data = df_clean[df_clean['Grade'] == grade]['Regional_Price']\n",
    "        ax4.hist(grade_data, alpha=0.6, label=grade, bins=20, density=True)\n",
    "    \n",
    "    ax4.set_title('Price Distribution Histograms by Grade', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Regional Price', fontsize=12)\n",
    "    ax4.set_ylabel('Density', fontsize=12)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPrice Distribution Summary by Grade:\")\n",
    "    print(\"=\" * 50)\n",
    "    summary_stats = df_clean.groupby('Grade')['Regional_Price'].describe()\n",
    "    print(summary_stats.round(2))\n",
    "\n",
    "\n",
    "def plot_feature_correlation_matrix(df):\n",
    "    \"\"\"Plot feature correlation matrix\"\"\"\n",
    "    numeric_features = [\n",
    "        'Regional_Price', 'National_Price', 'Seasonal_Impact',\n",
    "        'Local_Production_Volume', 'Local_Export_Volume',\n",
    "        'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "        'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate',\n",
    "        'Fuel_Price', 'Year', 'Month_num', 'Quarter', 'Grade_encoded',\n",
    "        'Region_encoded', 'Is_Active_Region'\n",
    "    ]\n",
    "    \n",
    "    available_features = [col for col in numeric_features if col in df.columns]\n",
    "    df_numeric = df[available_features].select_dtypes(include=[np.number])\n",
    "    correlation_matrix = df_numeric.corr()\n",
    "    \n",
    "    plt.figure(figsize=(16, 14))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                annot_kws={'size': 8})\n",
    "    \n",
    "    plt.title('Feature Correlation Matrix\\n(Cinnamon Price Forecasting Dataset)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    print(\"=\" * 60)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        for feature1, feature2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"{feature1} ↔ {feature2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"No feature pairs with |correlation| > 0.7 found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f8933",
   "metadata": {},
   "source": [
    "# 5. Sequence Peparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ba2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(df, sequence_length=12, target_col='Regional_Price'):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    feature_cols = [\n",
    "        'Grade_encoded', 'Region_encoded', 'Is_Active_Region',\n",
    "        'National_Price', 'Seasonal_Impact', 'Local_Production_Volume',\n",
    "        'Local_Export_Volume', 'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "        'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate', 'Fuel_Price',\n",
    "        'Year', 'Month_num', 'Quarter'\n",
    "    ]\n",
    "\n",
    "    # Add lag and rolling features\n",
    "    lag_cols = [col for col in df.columns if 'lag_' in col or 'rolling_' in col]\n",
    "    feature_cols.extend(lag_cols)\n",
    "\n",
    "    # Fill NaN values\n",
    "    df_clean = df.copy()\n",
    "    df_clean = df_clean.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    X_sequences, y_sequences, metadata = [], [], []\n",
    "\n",
    "    for grade in df_clean['Grade'].unique():\n",
    "        for region in df_clean['Region'].unique():\n",
    "            subset = df_clean[(df_clean['Grade'] == grade) & (df_clean['Region'] == region)].sort_values('Month')\n",
    "\n",
    "            if len(subset) < sequence_length + 1:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(subset) - sequence_length):\n",
    "                X_seq = subset.iloc[i:i + sequence_length][feature_cols].values\n",
    "                y_seq = subset.iloc[i + sequence_length][target_col]\n",
    "\n",
    "                X_sequences.append(X_seq)\n",
    "                y_sequences.append(y_seq)\n",
    "                metadata.append({\n",
    "                    'grade': grade,\n",
    "                    'region': region,\n",
    "                    'date': subset.iloc[i + sequence_length]['Month']\n",
    "                })\n",
    "\n",
    "    print(f\"Total sequences created: {len(X_sequences)}\")\n",
    "    return np.array(X_sequences), np.array(y_sequences), metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cfb29",
   "metadata": {},
   "source": [
    "# 6. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdba929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model_tunable(units1=128, units2=64, dropout1=0.2, dropout2=0.2, \n",
    "                            dense_units=32, optimizer='adam', learning_rate=0.001, \n",
    "                            layer_type='LSTM', use_batch_norm=False, input_shape=None):\n",
    "    \"\"\"Build tunable LSTM model\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First RNN layer\n",
    "    if layer_type == 'LSTM':\n",
    "        model.add(LSTM(units1, return_sequences=True, input_shape=input_shape))\n",
    "    elif layer_type == 'GRU':\n",
    "        model.add(GRU(units1, return_sequences=True, input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(SimpleRNN(units1, return_sequences=True, input_shape=input_shape))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout1))\n",
    "    \n",
    "    # Second RNN layer\n",
    "    if layer_type == 'LSTM':\n",
    "        model.add(LSTM(units2, return_sequences=False))\n",
    "    elif layer_type == 'GRU':\n",
    "        model.add(GRU(units2, return_sequences=False))\n",
    "    else:\n",
    "        model.add(SimpleRNN(units2, return_sequences=False))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout2))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Configure optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccbbb5",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebf3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"Hyperparameter tuning class\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, input_shape):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.input_shape = input_shape\n",
    "        self.best_params = None\n",
    "        self.best_score = float('inf')\n",
    "        self.tuning_results = []\n",
    "    \n",
    "    def optuna_tuning(self, n_trials=50):\n",
    "        \"\"\"Optuna-based hyperparameter tuning\"\"\"\n",
    "        print(\"\\n🎯 Starting Optuna Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'units1': trial.suggest_categorical('units1', [64, 128, 256, 512]),\n",
    "                'units2': trial.suggest_categorical('units2', [32, 64, 128, 256]),\n",
    "                'dropout1': trial.suggest_float('dropout1', 0.1, 0.5, step=0.1),\n",
    "                'dropout2': trial.suggest_float('dropout2', 0.1, 0.5, step=0.1),\n",
    "                'dense_units': trial.suggest_categorical('dense_units', [16, 32, 64, 128]),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n",
    "                'layer_type': trial.suggest_categorical('layer_type', ['LSTM', 'GRU']),\n",
    "                'use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),\n",
    "                'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                model = build_lstm_model_tunable(**params, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "                return val_loss\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Trial failed: {e}\")\n",
    "                return float('inf')\n",
    "        \n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        print(f\"\\n🎉 Optuna Tuning Complete!\")\n",
    "        print(f\"Best validation loss: {study.best_value:.6f}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        \n",
    "        return study.best_params, study.best_value\n",
    "    \n",
    "    def random_search_tuning(self, n_trials=30):\n",
    "        \"\"\"Random search hyperparameter tuning\"\"\"\n",
    "        print(\"\\n🎲 Starting Random Search Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for i in range(n_trials):\n",
    "            params = {\n",
    "                'units1': np.random.choice([64, 128, 256, 512]),\n",
    "                'units2': np.random.choice([32, 64, 128, 256]),\n",
    "                'dropout1': np.random.uniform(0.1, 0.5),\n",
    "                'dropout2': np.random.uniform(0.1, 0.5),\n",
    "                'dense_units': np.random.choice([16, 32, 64, 128]),\n",
    "                'learning_rate': 10 ** np.random.uniform(-4, -2),\n",
    "                'layer_type': np.random.choice(['LSTM', 'GRU']),\n",
    "                'use_batch_norm': np.random.choice([True, False]),\n",
    "                'optimizer': np.random.choice(['adam', 'rmsprop'])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nTrial {i+1}/{n_trials}\")\n",
    "                \n",
    "                model = build_lstm_model_tunable(**params, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                result = {\n",
    "                    'trial': i+1,\n",
    "                    'params': params.copy(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_mae': min(history.history['val_mae'])\n",
    "                }\n",
    "                \n",
    "                self.tuning_results.append(result)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = params.copy()\n",
    "                    print(f\"✅ New best validation loss: {val_loss:.6f}\")\n",
    "                else:\n",
    "                    print(f\"   Validation loss: {val_loss:.6f}\")\n",
    "                \n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Trial {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_params = best_params\n",
    "        self.best_score = best_val_loss\n",
    "        \n",
    "        print(f\"\\n🎉 Random Search Complete!\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        return best_params, best_val_loss\n",
    "\n",
    "\n",
    "def perform_hyperparameter_tuning(X_train, y_train, X_val, y_val, input_shape, \n",
    "                                 method='optuna', n_trials=30):\n",
    "    \"\"\"Main function to perform hyperparameter tuning\"\"\"\n",
    "    print(f\"\\n🚀 Starting Hyperparameter Tuning using {method.upper()} method...\")\n",
    "    \n",
    "    tuner = HyperparameterTuner(X_train, y_train, X_val, y_val, input_shape)\n",
    "    \n",
    "    if method == 'optuna':\n",
    "        best_params, best_score = tuner.optuna_tuning(n_trials=n_trials)\n",
    "    elif method == 'random':\n",
    "        best_params, best_score = tuner.random_search_tuning(n_trials=n_trials)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'optuna' or 'random'\")\n",
    "    \n",
    "    return best_params, best_score, tuner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
