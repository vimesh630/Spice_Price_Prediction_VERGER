{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26627d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from itertools import product\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEQUENCE_LENGTH = 12  # 12 months lookback\n",
    "MODEL_DIR = 'cinnamon_models'\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    print(f\"Created model directory: {MODEL_DIR}\")\n",
    "\n",
    "# Initialize preprocessors\n",
    "scaler_features = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "label_encoders = {}\n",
    "\n",
    "def load_and_prepare_data(data_path):\n",
    "    \"\"\"Load and prepare the cinnamon price dataset\"\"\"\n",
    "    # Load data\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Convert Month to datetime\n",
    "    df['Month'] = pd.to_datetime(df['Month'])\n",
    "\n",
    "    # Handle missing values in Regional_Price\n",
    "    missing_before = df['Regional_Price'].isna().sum()\n",
    "    df.loc[df['Is_Active_Region'] == 0, 'Regional_Price'] = df.loc[df['Is_Active_Region'] == 0, 'National_Price']\n",
    "    missing_after = df['Regional_Price'].isna().sum()\n",
    "    print(f\"Missing Regional_Price values: {missing_before} -> {missing_after}\")\n",
    "\n",
    "    # Encode categorical variables\n",
    "    for col in ['Grade', 'Region']:\n",
    "        if col not in label_encoders:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "    # Create additional time-based features\n",
    "    df['Year'] = df['Month'].dt.year\n",
    "    df['Month_num'] = df['Month'].dt.month\n",
    "    df['Quarter'] = df['Month'].dt.quarter\n",
    "\n",
    "    print(\"Creating lag and rolling features...\")\n",
    "\n",
    "    # Create lag features for key variables\n",
    "    df = df.sort_values(['Grade', 'Region', 'Month'])\n",
    "    lag_columns = ['Regional_Price', 'National_Price', 'Temperature', 'Rainfall']\n",
    "    for col in lag_columns:\n",
    "        if col in df.columns:\n",
    "            for lag in [1, 3, 6, 12]:\n",
    "                df[f'{col}_lag_{lag}'] = df.groupby(['Grade', 'Region'])[col].shift(lag)\n",
    "\n",
    "    # Create rolling averages\n",
    "    for col in ['Regional_Price', 'Temperature', 'Rainfall']:\n",
    "        if col in df.columns:\n",
    "            for window in [3, 6, 12]:\n",
    "                df[f'{col}_rolling_{window}'] = df.groupby(['Grade', 'Region'])[col].transform(\n",
    "                    lambda x: x.rolling(window).mean()\n",
    "                )\n",
    "\n",
    "    print(f\"Final data shape after feature engineering: {df.shape}\")\n",
    "    print(f\"Unique grades: {df['Grade'].unique()}\")\n",
    "    print(f\"Unique regions: {df['Region'].unique()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "DATA_PATH = 'Cinnamon_Dataset_New_0001.csv'\n",
    "\n",
    "df = load_and_prepare_data(DATA_PATH)\n",
    "print(\"\\nData loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n",
    "\n",
    "def plot_price_distribution_by_grade(df):\n",
    "    \"\"\"Plot price distribution by grade\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Remove rows with missing Regional_Price\n",
    "    df_clean = df.dropna(subset=['Regional_Price'])\n",
    "    \n",
    "    # Create subplots for different visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Box plot of prices by grade\n",
    "    sns.boxplot(data=df_clean, x='Grade', y='Regional_Price', ax=ax1)\n",
    "    ax1.set_title('Price Distribution by Grade (Box Plot)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Grade', fontsize=12)\n",
    "    ax1.set_ylabel('Regional Price', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Violin plot for detailed distribution\n",
    "    sns.violinplot(data=df_clean, x='Grade', y='Regional_Price', ax=ax2)\n",
    "    ax2.set_title('Price Distribution by Grade (Violin Plot)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Grade', fontsize=12)\n",
    "    ax2.set_ylabel('Regional Price', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Mean prices by grade with error bars\n",
    "    grade_stats = df_clean.groupby('Grade')['Regional_Price'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    grade_stats['se'] = grade_stats['std'] / np.sqrt(grade_stats['count'])\n",
    "    \n",
    "    ax3.bar(grade_stats['Grade'], grade_stats['mean'], \n",
    "            yerr=grade_stats['se'], capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    ax3.set_title('Average Price by Grade (with Standard Error)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Grade', fontsize=12)\n",
    "    ax3.set_ylabel('Average Regional Price', fontsize=12)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (grade, mean_price) in enumerate(zip(grade_stats['Grade'], grade_stats['mean'])):\n",
    "        ax3.text(i, mean_price + grade_stats['se'].iloc[i], f'{mean_price:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Histogram of all prices colored by grade\n",
    "    for grade in df_clean['Grade'].unique():\n",
    "        grade_data = df_clean[df_clean['Grade'] == grade]['Regional_Price']\n",
    "        ax4.hist(grade_data, alpha=0.6, label=grade, bins=20, density=True)\n",
    "    \n",
    "    ax4.set_title('Price Distribution Histograms by Grade', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Regional Price', fontsize=12)\n",
    "    ax4.set_ylabel('Density', fontsize=12)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nPrice Distribution Summary by Grade:\")\n",
    "    print(\"=\" * 50)\n",
    "    summary_stats = df_clean.groupby('Grade')['Regional_Price'].describe()\n",
    "    print(summary_stats.round(2))\n",
    "\n",
    "plot_price_distribution_by_grade(df)\n",
    "\n",
    "def plot_feature_correlation_matrix(df):\n",
    "    \"\"\"Plot feature correlation matrix\"\"\"\n",
    "    # Select numeric features for correlation analysis\n",
    "    numeric_features = [\n",
    "        'Regional_Price', 'National_Price', 'Seasonal_Impact',\n",
    "        'Local_Production_Volume', 'Local_Export_Volume',\n",
    "        'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "        'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate',\n",
    "        'Fuel_Price', 'Year', 'Month_num', 'Quarter', 'Grade_encoded',\n",
    "        'Region_encoded', 'Is_Active_Region','GRN_QTY'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    available_features = [col for col in numeric_features if col in df.columns]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    df_numeric = df[available_features].select_dtypes(include=[np.number])\n",
    "    correlation_matrix = df_numeric.corr()\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    \n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                annot_kws={'size': 8})\n",
    "    \n",
    "    plt.title('Feature Correlation Matrix\\n(Cinnamon Price Forecasting Dataset)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print highly correlated feature pairs\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    print(\"=\" * 60)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        for feature1, feature2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"{feature1} â†” {feature2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"No feature pairs with |correlation| > 0.7 found.\")\n",
    "\n",
    "plot_feature_correlation_matrix(df)\n",
    "\n",
    "def prepare_sequences(df, sequence_length=12, target_col='Regional_Price'):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    feature_cols = [\n",
    "        'Grade_encoded', 'Region_encoded', 'Is_Active_Region',\n",
    "        'National_Price', 'Seasonal_Impact', 'Local_Production_Volume',\n",
    "        'Local_Export_Volume', 'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "        'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate', 'Fuel_Price',\n",
    "        'Year', 'Month_num', 'Quarter'\n",
    "    ]\n",
    "\n",
    "    # Add lag and rolling features\n",
    "    lag_cols = [col for col in df.columns if 'lag_' in col or 'rolling_' in col]\n",
    "    feature_cols.extend(lag_cols)\n",
    "\n",
    "    # Instead of dropping all NaNs, fill them\n",
    "    df_clean = df.copy()\n",
    "    df_clean = df_clean.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    X_sequences, y_sequences, metadata = [], [], []\n",
    "\n",
    "    for grade in df_clean['Grade'].unique():\n",
    "        for region in df_clean['Region'].unique():\n",
    "            subset = df_clean[(df_clean['Grade'] == grade) & (df_clean['Region'] == region)].sort_values('Month')\n",
    "\n",
    "            if len(subset) < sequence_length + 1:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(subset) - sequence_length):\n",
    "                X_seq = subset.iloc[i:i + sequence_length][feature_cols].values\n",
    "                y_seq = subset.iloc[i + sequence_length][target_col]\n",
    "\n",
    "                X_sequences.append(X_seq)\n",
    "                y_sequences.append(y_seq)\n",
    "                metadata.append({\n",
    "                    'grade': grade,\n",
    "                    'region': region,\n",
    "                    'date': subset.iloc[i + sequence_length]['Month']\n",
    "                })\n",
    "\n",
    "    print(\"Total sequences created:\", len(X_sequences))\n",
    "    return np.array(X_sequences), np.array(y_sequences), metadata\n",
    "\n",
    "# ============ HYPERPARAMETER TUNING SECTION ============\n",
    "\n",
    "def build_lstm_model_tunable(units1=128, units2=64, dropout1=0.2, dropout2=0.2, \n",
    "                            dense_units=32, optimizer='adam', learning_rate=0.001, \n",
    "                            layer_type='LSTM', use_batch_norm=False, input_shape=None):\n",
    "    \"\"\"Build tunable LSTM model with various hyperparameters\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Choose layer type\n",
    "    if layer_type == 'LSTM':\n",
    "        model.add(LSTM(units1, return_sequences=True, input_shape=input_shape))\n",
    "    elif layer_type == 'GRU':\n",
    "        model.add(GRU(units1, return_sequences=True, input_shape=input_shape))\n",
    "    else:  # SimpleRNN\n",
    "        model.add(SimpleRNN(units1, return_sequences=True, input_shape=input_shape))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dropout(dropout1))\n",
    "    \n",
    "    # Second RNN layer\n",
    "    if layer_type == 'LSTM':\n",
    "        model.add(LSTM(units2, return_sequences=False))\n",
    "    elif layer_type == 'GRU':\n",
    "        model.add(GRU(units2, return_sequences=False))\n",
    "    else:  # SimpleRNN\n",
    "        model.add(SimpleRNN(units2, return_sequences=False))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    model.add(Dropout(dropout2))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Configure optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:  # SGD\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"Hyperparameter tuning class using multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, input_shape):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.input_shape = input_shape\n",
    "        self.best_params = None\n",
    "        self.best_score = float('inf')\n",
    "        self.tuning_results = []\n",
    "    \n",
    "    def grid_search_tuning(self, param_grid=None, max_trials=20):\n",
    "        \"\"\"Grid search hyperparameter tuning\"\"\"\n",
    "        print(\"\\nðŸ” Starting Grid Search Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'units1': [64, 128, 256],\n",
    "                'units2': [32, 64, 128],\n",
    "                'dropout1': [0.1, 0.2, 0.3],\n",
    "                'dropout2': [0.1, 0.2, 0.3],\n",
    "                'dense_units': [16, 32, 64],\n",
    "                'learning_rate': [0.001, 0.0005, 0.002],\n",
    "                'layer_type': ['LSTM', 'GRU'],\n",
    "                'use_batch_norm': [True, False]\n",
    "            }\n",
    "        \n",
    "        # Generate all combinations and sample randomly if too many\n",
    "        param_combinations = list(product(*param_grid.values()))\n",
    "        if len(param_combinations) > max_trials:\n",
    "            param_combinations = np.random.choice(\n",
    "                param_combinations, size=max_trials, replace=False\n",
    "            )\n",
    "        \n",
    "        print(f\"Testing {len(param_combinations)} parameter combinations...\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for i, params in enumerate(param_combinations[:max_trials]):\n",
    "            param_dict = dict(zip(param_grid.keys(), params))\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nTrial {i+1}/{min(len(param_combinations), max_trials)}: {param_dict}\")\n",
    "                \n",
    "                # Build and train model\n",
    "                model = build_lstm_model_tunable(**param_dict, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                result = {\n",
    "                    'trial': i+1,\n",
    "                    'params': param_dict.copy(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_mae': min(history.history['val_mae'])\n",
    "                }\n",
    "                \n",
    "                self.tuning_results.append(result)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = param_dict.copy()\n",
    "                    print(f\"âœ… New best validation loss: {val_loss:.6f}\")\n",
    "                else:\n",
    "                    print(f\"   Validation loss: {val_loss:.6f}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Trial {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_params = best_params\n",
    "        self.best_score = best_val_loss\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Grid Search Complete!\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        return best_params, best_val_loss\n",
    "    \n",
    "    def optuna_tuning(self, n_trials=50):\n",
    "        \"\"\"Optuna-based hyperparameter tuning\"\"\"\n",
    "        print(\"\\nðŸŽ¯ Starting Optuna Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Define hyperparameter search space\n",
    "            params = {\n",
    "                'units1': trial.suggest_categorical('units1', [64, 128, 256, 512]),\n",
    "                'units2': trial.suggest_categorical('units2', [32, 64, 128, 256]),\n",
    "                'dropout1': trial.suggest_float('dropout1', 0.1, 0.5, step=0.1),\n",
    "                'dropout2': trial.suggest_float('dropout2', 0.1, 0.5, step=0.1),\n",
    "                'dense_units': trial.suggest_categorical('dense_units', [16, 32, 64, 128]),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n",
    "                'layer_type': trial.suggest_categorical('layer_type', ['LSTM', 'GRU']),\n",
    "                'use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),\n",
    "                'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                model = build_lstm_model_tunable(**params, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "                return val_loss\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Trial failed: {e}\")\n",
    "                return float('inf')\n",
    "        \n",
    "        # Create study and optimize\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Optuna Tuning Complete!\")\n",
    "        print(f\"Best validation loss: {study.best_value:.6f}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        \n",
    "        # Plot optimization history\n",
    "        self.plot_optuna_results(study)\n",
    "        \n",
    "        return study.best_params, study.best_value\n",
    "    \n",
    "    def plot_optuna_results(self, study):\n",
    "        \"\"\"Plot Optuna optimization results\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot optimization history\n",
    "        trials = study.trials\n",
    "        values = [t.value for t in trials if t.value is not None]\n",
    "        ax1.plot(values, marker='o', alpha=0.7)\n",
    "        ax1.set_title('Optimization History', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Trial')\n",
    "        ax1.set_ylabel('Validation Loss')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot parameter importance\n",
    "        if len(study.trials) > 10:\n",
    "            try:\n",
    "                importance = optuna.importance.get_param_importances(study)\n",
    "                params = list(importance.keys())\n",
    "                importances = list(importance.values())\n",
    "                \n",
    "                ax2.barh(params, importances)\n",
    "                ax2.set_title('Parameter Importance', fontsize=14, fontweight='bold')\n",
    "                ax2.set_xlabel('Importance')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "            except:\n",
    "                ax2.text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                        ha='center', va='center', transform=ax2.transAxes)\n",
    "        \n",
    "        # Plot best value progression\n",
    "        best_values = []\n",
    "        current_best = float('inf')\n",
    "        for trial in study.trials:\n",
    "            if trial.value is not None and trial.value < current_best:\n",
    "                current_best = trial.value\n",
    "            best_values.append(current_best)\n",
    "        \n",
    "        ax3.plot(best_values, marker='o', alpha=0.7, color='green')\n",
    "        ax3.set_title('Best Value Progression', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Trial')\n",
    "        ax3.set_ylabel('Best Validation Loss')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Distribution of validation losses\n",
    "        ax4.hist(values, bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(study.best_value, color='red', linestyle='--', \n",
    "                   label=f'Best: {study.best_value:.6f}')\n",
    "        ax4.set_title('Distribution of Validation Losses', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Validation Loss')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def random_search_tuning(self, n_trials=30):\n",
    "        \"\"\"Random search hyperparameter tuning\"\"\"\n",
    "        print(\"\\nðŸŽ² Starting Random Search Hyperparameter Tuning...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for i in range(n_trials):\n",
    "            # Randomly sample hyperparameters\n",
    "            params = {\n",
    "                'units1': np.random.choice([64, 128, 256, 512]),\n",
    "                'units2': np.random.choice([32, 64, 128, 256]),\n",
    "                'dropout1': np.random.uniform(0.1, 0.5),\n",
    "                'dropout2': np.random.uniform(0.1, 0.5),\n",
    "                'dense_units': np.random.choice([16, 32, 64, 128]),\n",
    "                'learning_rate': np.random.loguniform(1e-4, 1e-2),\n",
    "                'layer_type': np.random.choice(['LSTM', 'GRU']),\n",
    "                'use_batch_norm': np.random.choice([True, False]),\n",
    "                'optimizer': np.random.choice(['adam', 'rmsprop'])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nTrial {i+1}/{n_trials}: {params}\")\n",
    "                \n",
    "                model = build_lstm_model_tunable(**params, input_shape=self.input_shape)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.X_train, self.y_train,\n",
    "                    validation_data=(self.X_val, self.y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "                        EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=3, factor=0.5, verbose=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                result = {\n",
    "                    'trial': i+1,\n",
    "                    'params': params.copy(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_mae': min(history.history['val_mae'])\n",
    "                }\n",
    "                \n",
    "                self.tuning_results.append(result)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_params = params.copy()\n",
    "                    print(f\"âœ… New best validation loss: {val_loss:.6f}\")\n",
    "                else:\n",
    "                    print(f\"   Validation loss: {val_loss:.6f}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Trial {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_params = best_params\n",
    "        self.best_score = best_val_loss\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Random Search Complete!\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        return best_params, best_val_loss\n",
    "    \n",
    "    def plot_tuning_results(self):\n",
    "        \"\"\"Plot hyperparameter tuning results\"\"\"\n",
    "        if not self.tuning_results:\n",
    "            print(\"No tuning results to plot.\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Extract data\n",
    "        trials = [r['trial'] for r in self.tuning_results]\n",
    "        val_losses = [r['val_loss'] for r in self.tuning_results]\n",
    "        val_maes = [r['val_mae'] for r in self.tuning_results]\n",
    "        \n",
    "        # Plot validation loss progression\n",
    "        ax1.plot(trials, val_losses, marker='o', alpha=0.7)\n",
    "        ax1.set_title('Validation Loss by Trial', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Trial')\n",
    "        ax1.set_ylabel('Validation Loss')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot validation MAE progression\n",
    "        ax2.plot(trials, val_maes, marker='s', alpha=0.7, color='orange')\n",
    "        ax2.set_title('Validation MAE by Trial', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Trial')\n",
    "        ax2.set_ylabel('Validation MAE')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Histogram of validation losses\n",
    "        ax3.hist(val_losses, bins=15, alpha=0.7, edgecolor='black')\n",
    "        ax3.axvline(self.best_score, color='red', linestyle='--', \n",
    "                   label=f'Best: {self.best_score:.6f}')\n",
    "        ax3.set_title('Distribution of Validation Losses', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Validation Loss')\n",
    "        ax3.set_ylabel('Frequency')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Best value progression\n",
    "        best_so_far = []\n",
    "        current_best = float('inf')\n",
    "        for loss in val_losses:\n",
    "            if loss < current_best:\n",
    "                current_best = loss\n",
    "            best_so_far.append(current_best)\n",
    "        \n",
    "        ax4.plot(trials, best_so_far, marker='o', alpha=0.7, color='green')\n",
    "        ax4.set_title('Best Validation Loss Progression', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Trial')\n",
    "        ax4.set_ylabel('Best Validation Loss')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def perform_hyperparameter_tuning(X_train, y_train, X_val, y_val, input_shape, \n",
    "                                 method='optuna', n_trials=30):\n",
    "    \"\"\"Main function to perform hyperparameter tuning\"\"\"\n",
    "    print(f\"\\nðŸš€ Starting Hyperparameter Tuning using {method.upper()} method...\")\n",
    "    \n",
    "    tuner = HyperparameterTuner(X_train, y_train, X_val, y_val, input_shape)\n",
    "    \n",
    "    if method == 'optuna':\n",
    "        best_params, best_score = tuner.optuna_tuning(n_trials=n_trials)\n",
    "    elif method == 'random':\n",
    "        best_params, best_score = tuner.random_search_tuning(n_trials=n_trials)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'optuna', 'grid', or 'random'\")\n",
    "    \n",
    "    # Plot results\n",
    "    if method in ['grid', 'random']:\n",
    "        tuner.plot_tuning_results()\n",
    "    \n",
    "    return best_params, best_score, tuner\n",
    "\n",
    "# ============ END HYPERPARAMETER TUNING SECTION ============\n",
    "\n",
    "def build_lstm_model(input_shape, best_params=None):\n",
    "    \"\"\"Build LSTM model with optional best parameters from tuning\"\"\"\n",
    "    if best_params is None:\n",
    "        # Default parameters\n",
    "        best_params = {\n",
    "            'units1': 128,\n",
    "            'units2': 64,\n",
    "            'dropout1': 0.2,\n",
    "            'dropout2': 0.2,\n",
    "            'dense_units': 32,\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'layer_type': 'LSTM',\n",
    "            'use_batch_norm': False\n",
    "        }\n",
    "    \n",
    "    return build_lstm_model_tunable(**best_params, input_shape=input_shape)\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training vs validation loss\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "    ax1.set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training & validation MAE\n",
    "    ax2.plot(history.history['mae'], label='Training MAE', linewidth=2, color='blue')\n",
    "    ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2, color='red')\n",
    "    ax2.set_title('Model MAE Over Epochs', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('MAE', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_mae = history.history['mae'][-1]\n",
    "    final_val_mae = history.history['val_mae'][-1]\n",
    "    \n",
    "    print(f\"\\nFinal Training Metrics:\")\n",
    "    print(f\"Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Training MAE: {final_train_mae:.4f}\")\n",
    "    print(f\"Validation MAE: {final_val_mae:.4f}\")\n",
    "\n",
    "def train_model(df, use_tuning=True, tuning_method='optuna', n_tuning_trials=20):\n",
    "    \"\"\"Train the forecasting model with optional hyperparameter tuning\"\"\"\n",
    "    global scaler_features, scaler_target\n",
    "    \n",
    "    print(\"Preparing sequences...\")\n",
    "    X, y, metadata = prepare_sequences(df, SEQUENCE_LENGTH)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No sequences could be created. Check if there's enough data.\")\n",
    "\n",
    "    print(f\"Created {len(X)} sequences with shape {X.shape}\")\n",
    "\n",
    "    # Scale features and target\n",
    "    print(\"Scaling features...\")\n",
    "    n_samples, n_timesteps, n_features = X.shape\n",
    "    X_reshaped = X.reshape(-1, n_features)\n",
    "    X_scaled_reshaped = scaler_features.fit_transform(X_reshaped)\n",
    "    X_scaled = X_scaled_reshaped.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "    y_scaled = scaler_target.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Train-validation-test split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2 of total\n",
    "    )\n",
    "\n",
    "    print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Validation set shape: X={X_val.shape}, y={y_val.shape}\")\n",
    "    print(f\"Test set shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    best_params = None\n",
    "    tuner = None\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    if use_tuning:\n",
    "        print(f\"\\nðŸ”§ Performing hyperparameter tuning using {tuning_method} method...\")\n",
    "        best_params, best_score, tuner = perform_hyperparameter_tuning(\n",
    "            X_train, y_train, X_val, y_val, input_shape, \n",
    "            method=tuning_method, n_trials=n_tuning_trials\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Hyperparameter Tuning Results:\")\n",
    "        print(f\"Best validation loss: {best_score:.6f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping hyperparameter tuning, using default parameters...\")\n",
    "\n",
    "    # Build and train final model with best parameters\n",
    "    print(\"\\nBuilding final model with optimized parameters...\")\n",
    "    model = build_lstm_model(input_shape, best_params)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"\\nðŸ“‹ Final Model Architecture:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nTraining final model...\")\n",
    "    \n",
    "    # Use longer training for final model\n",
    "    final_epochs = 150 if use_tuning else 100\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=final_epochs,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=15, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(patience=8, factor=0.5, verbose=1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_unscaled = scaler_target.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_unscaled = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))\n",
    "    r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Final Model Performance on Test Set:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    # Create comprehensive results dictionary\n",
    "    results = {\n",
    "        'mae': mae, \n",
    "        'rmse': rmse, \n",
    "        'r2': r2,\n",
    "        'best_params': best_params,\n",
    "        'tuning_method': tuning_method if use_tuning else None,\n",
    "        'tuning_used': use_tuning,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'final_train_loss': history.history['loss'][-1],\n",
    "        'final_val_loss': history.history['val_loss'][-1]\n",
    "    }\n",
    "    \n",
    "    # Add tuning results if available\n",
    "    if tuner and hasattr(tuner, 'tuning_results'):\n",
    "        results['tuning_results'] = tuner.tuning_results\n",
    "    \n",
    "    return model, history, results\n",
    "\n",
    "# Train model with hyperparameter tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ STARTING MODEL TRAINING WITH HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# You can change these parameters:\n",
    "USE_HYPERPARAMETER_TUNING = True  # Set to False to skip tuning\n",
    "TUNING_METHOD = 'optuna'  # Options: 'optuna', 'grid', 'random'\n",
    "N_TUNING_TRIALS = 25  # Number of trials for tuning\n",
    "\n",
    "model, history, metrics = train_model(\n",
    "    df, \n",
    "    use_tuning=USE_HYPERPARAMETER_TUNING,\n",
    "    tuning_method=TUNING_METHOD,\n",
    "    n_tuning_trials=N_TUNING_TRIALS\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Model training completed!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def save_model(model, metrics, model_dir=MODEL_DIR):\n",
    "    \"\"\"Save the trained model and all preprocessors\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create model-specific directory\n",
    "    model_save_dir = os.path.join(model_dir, f\"cinnamon_model_{timestamp}\")\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Saving model to: {model_save_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Save the Keras model\n",
    "        model_path = os.path.join(model_save_dir, \"lstm_model.keras\")\n",
    "        model.save(model_path)\n",
    "        print(f\"âœ… Keras model saved: lstm_model.keras\")\n",
    "        \n",
    "        # 2. Save scalers\n",
    "        scalers_path = os.path.join(model_save_dir, \"scalers.pkl\")\n",
    "        scalers = {\n",
    "            'scaler_features': scaler_features,\n",
    "            'scaler_target': scaler_target\n",
    "        }\n",
    "        with open(scalers_path, 'wb') as f:\n",
    "            pickle.dump(scalers, f)\n",
    "        print(f\"âœ… Scalers saved: scalers.pkl\")\n",
    "        \n",
    "        # 3. Save label encoders\n",
    "        encoders_path = os.path.join(model_save_dir, \"label_encoders.pkl\")\n",
    "        with open(encoders_path, 'wb') as f:\n",
    "            pickle.dump(label_encoders, f)\n",
    "        print(f\"âœ… Label encoders saved: label_encoders.pkl\")\n",
    "        \n",
    "        # 4. Save model configuration and metadata (enhanced with tuning info)\n",
    "        config = {\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "            'model_architecture': {\n",
    "                'input_shape': model.input_shape,\n",
    "                'layers': [str(layer.__class__.__name__) for layer in model.layers],\n",
    "                'total_params': model.count_params()\n",
    "            },\n",
    "            'training_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'mae': float(metrics['mae']),\n",
    "                'rmse': float(metrics['rmse']),\n",
    "                'r2': float(metrics['r2']),\n",
    "                'tuning_used': metrics.get('tuning_used', False),\n",
    "                'tuning_method': metrics.get('tuning_method', None),\n",
    "                'best_params': metrics.get('best_params', None),\n",
    "                'epochs_trained': metrics.get('epochs_trained', 0),\n",
    "                'final_train_loss': float(metrics.get('final_train_loss', 0)),\n",
    "                'final_val_loss': float(metrics.get('final_val_loss', 0))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save tuning results if available\n",
    "        if 'tuning_results' in metrics and metrics['tuning_results']:\n",
    "            tuning_results_path = os.path.join(model_save_dir, \"tuning_results.json\")\n",
    "            with open(tuning_results_path, 'w') as f:\n",
    "                json.dump(metrics['tuning_results'], f, indent=2, default=str)\n",
    "            print(f\"âœ… Hyperparameter tuning results saved: tuning_results.json\")\n",
    "        \n",
    "        config_path = os.path.join(model_save_dir, \"model_config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2, default=str)\n",
    "        print(f\"âœ… Model configuration saved: model_config.json\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Model successfully saved to: {model_save_dir}\")\n",
    "        \n",
    "        # Print summary of saved model\n",
    "        print(f\"\\nðŸ“Š Saved Model Summary:\")\n",
    "        print(f\"  â€¢ Performance: MAE={metrics['mae']:.2f}, RMSE={metrics['rmse']:.2f}, RÂ²={metrics['r2']:.4f}\")\n",
    "        if metrics.get('tuning_used'):\n",
    "            print(f\"  â€¢ Hyperparameter tuning: {metrics['tuning_method']} method used\")\n",
    "            print(f\"  â€¢ Best parameters found and applied\")\n",
    "        print(f\"  â€¢ Training epochs: {metrics.get('epochs_trained', 'N/A')}\")\n",
    "        print(f\"  â€¢ Total parameters: {model.count_params():,}\")\n",
    "        \n",
    "        return model_save_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_saved_model(model_path):\n",
    "    \"\"\"Load a previously saved model and preprocessors\"\"\"\n",
    "    global scaler_features, scaler_target, label_encoders\n",
    "    \n",
    "    print(f\"ðŸ“‚ Loading model from: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the Keras model\n",
    "        keras_model_path = os.path.join(model_path, \"lstm_model.keras\")\n",
    "        model = load_model(keras_model_path)\n",
    "        print(f\"âœ… Keras model loaded\")\n",
    "        \n",
    "        # Load scalers\n",
    "        scalers_path = os.path.join(model_path, \"scalers.pkl\")\n",
    "        with open(scalers_path, 'rb') as f:\n",
    "            scalers = pickle.load(f)\n",
    "        scaler_features = scalers['scaler_features']\n",
    "        scaler_target = scalers['scaler_target']\n",
    "        print(f\"âœ… Scalers loaded\")\n",
    "        \n",
    "        # Load label encoders\n",
    "        encoders_path = os.path.join(model_path, \"label_encoders.pkl\")\n",
    "        with open(encoders_path, 'rb') as f:\n",
    "            label_encoders = pickle.load(f)\n",
    "        print(f\"âœ… Label encoders loaded\")\n",
    "        \n",
    "        # Load configuration\n",
    "        config_path = os.path.join(model_path, \"model_config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        print(f\"ðŸŽ‰ Model successfully loaded!\")\n",
    "        print(f\"ðŸ“Š Performance: MAE={config['training_info']['mae']:.2f}, \"\n",
    "              f\"RMSE={config['training_info']['rmse']:.2f}, \"\n",
    "              f\"RÂ²={config['training_info']['r2']:.4f}\")\n",
    "        \n",
    "        if config['training_info'].get('tuning_used'):\n",
    "            print(f\"ðŸ”§ This model was trained with {config['training_info']['tuning_method']} hyperparameter tuning\")\n",
    "        \n",
    "        return model, config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "saved_model_path=save_model(model, metrics)\n",
    "print(f\"Model saved at: {saved_model_path}\")\n",
    "\n",
    "# Define TRAIN_FEATURE_COLS globally when preparing training data\n",
    "TRAIN_FEATURE_COLS = [\n",
    "    'Grade_encoded', 'Region_encoded', 'Is_Active_Region',\n",
    "    'National_Price', 'Seasonal_Impact', 'Local_Production_Volume',\n",
    "    'Local_Export_Volume', 'Global_Production_Volume', 'Global_Consumption_Volume',\n",
    "    'Temperature', 'Rainfall', 'Exchange_Rate', 'Inflation_Rate', 'Fuel_Price',\n",
    "    'Year', 'Month_num', 'Quarter',\n",
    "    'Regional_Price_lag_1', 'Regional_Price_lag_3', 'Regional_Price_lag_6', 'Regional_Price_lag_12',\n",
    "    'National_Price_lag_1', 'National_Price_lag_3', 'National_Price_lag_6', 'National_Price_lag_12',\n",
    "    'Temperature_lag_1', 'Temperature_lag_3', 'Temperature_lag_6', 'Temperature_lag_12',\n",
    "    'Rainfall_lag_1', 'Rainfall_lag_3', 'Rainfall_lag_6', 'Rainfall_lag_12',\n",
    "    'Regional_Price_rolling_3', 'Regional_Price_rolling_6', 'Regional_Price_rolling_12',\n",
    "    'Temperature_rolling_3', 'Temperature_rolling_6', 'Temperature_rolling_12',\n",
    "    'Rainfall_rolling_3', 'Rainfall_rolling_6', 'Rainfall_rolling_12'\n",
    "]\n",
    "\n",
    "def forecast_prices(model, df, grade, region, months_ahead=12):\n",
    "    subset = df[(df['Grade'] == grade) & (df['Region'] == region)].sort_values('Month')\n",
    "    last_row = subset.iloc[-1]\n",
    "    last_date = last_row['Month']\n",
    "\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1),\n",
    "                                 periods=months_ahead, freq='MS')\n",
    "    \n",
    "    # Generate future rows\n",
    "    future_rows = []\n",
    "    for future_date in future_dates:\n",
    "        row = last_row.copy()\n",
    "        row['Month'] = future_date\n",
    "        row['Year'] = future_date.year\n",
    "        row['Month_num'] = future_date.month\n",
    "        row['Quarter'] = future_date.quarter\n",
    "        row['Temperature'] = last_row['Temperature'] + 2 * np.sin(2*np.pi*(future_date.month-1)/12) + np.random.normal(0,0.5)\n",
    "        row['Rainfall'] = max(0, last_row['Rainfall'] + 20 * np.sin(2*np.pi*(future_date.month-1)/12) + np.random.normal(0,10))\n",
    "        row['Exchange_Rate'] = last_row['Exchange_Rate'] * (1 + np.random.normal(0.001,0.005))\n",
    "        row['Inflation_Rate'] = last_row['Inflation_Rate'] + np.random.normal(0,0.1)\n",
    "        row['Fuel_Price'] = last_row['Fuel_Price'] * (1 + np.random.normal(0.002,0.02))\n",
    "        future_rows.append(row)\n",
    "\n",
    "    future_df = pd.DataFrame(future_rows)\n",
    "    extended_df = pd.concat([subset, future_df], ignore_index=True).sort_values('Month')\n",
    "\n",
    "    # Recreate lag and rolling features\n",
    "    for col in ['Regional_Price','National_Price','Temperature','Rainfall']:\n",
    "        for lag in [1,3,6,12]:\n",
    "            extended_df[f'{col}_lag_{lag}'] = extended_df.groupby(['Grade','Region'])[col].shift(lag)\n",
    "        for window in [3,6,12]:\n",
    "            extended_df[f'{col}_rolling_{window}'] = extended_df.groupby(['Grade','Region'])[col].transform(lambda x: x.rolling(window).mean())\n",
    "\n",
    "    # Select exactly the features used during training\n",
    "    feature_cols = [c for c in TRAIN_FEATURE_COLS if c in extended_df.columns]\n",
    "\n",
    "    forecasts = []\n",
    "    historical_data = extended_df[extended_df['Month'] <= last_date]\n",
    "\n",
    "    for i in range(months_ahead):\n",
    "        current_data = extended_df.iloc[len(historical_data)-SEQUENCE_LENGTH+i : len(historical_data)+i]\n",
    "        if len(current_data) < SEQUENCE_LENGTH:\n",
    "            padding_needed = SEQUENCE_LENGTH - len(current_data)\n",
    "            last_known = historical_data.iloc[-1:].copy()\n",
    "            padding_data = pd.concat([last_known]*padding_needed, ignore_index=True)\n",
    "            current_data = pd.concat([padding_data, current_data], ignore_index=True).iloc[-SEQUENCE_LENGTH:]\n",
    "\n",
    "        sequence = current_data[feature_cols].ffill().bfill().values\n",
    "        sequence_flat = sequence.reshape(-1, sequence.shape[-1])\n",
    "        sequence_scaled_flat = scaler_features.transform(sequence_flat)\n",
    "        sequence_scaled = sequence_scaled_flat.reshape(sequence.shape)\n",
    "\n",
    "        next_pred = model.predict(sequence_scaled.reshape(1, SEQUENCE_LENGTH, -1), verbose=0)\n",
    "        next_pred_unscaled = scaler_target.inverse_transform(next_pred)[0][0]\n",
    "        forecasts.append(next_pred_unscaled)\n",
    "\n",
    "        future_idx = len(historical_data)+i\n",
    "        extended_df.iloc[future_idx, extended_df.columns.get_loc('Regional_Price')] = next_pred_unscaled\n",
    "        extended_df.iloc[future_idx, extended_df.columns.get_loc('National_Price')] = next_pred_unscaled\n",
    "\n",
    "    return forecasts, future_dates\n",
    "\n",
    "def plot_forecast_results(df, model, grade, region, months_ahead=12):\n",
    "    \"\"\"Plot 2: Historical data with forecast results\"\"\"\n",
    "    try:\n",
    "        # Get historical data for the specific grade and region\n",
    "        subset = df[(df['Grade'] == grade) & (df['Region'] == region)].sort_values('Month')\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            print(f\"No data found for {grade} in {region}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts, future_dates = forecast_prices(model, df, grade, region, months_ahead)\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        \n",
    "        # Plot historical data\n",
    "        plt.plot(subset['Month'], subset['Regional_Price'], \n",
    "                label='Historical Prices', linewidth=2, color='blue', marker='o', markersize=4)\n",
    "        \n",
    "        # CREATE BRIDGE: Connect last historical point to first forecast\n",
    "        last_historical_date = subset['Month'].iloc[-1]\n",
    "        last_historical_price = subset['Regional_Price'].iloc[-1]\n",
    "        first_forecast_date = future_dates[0]\n",
    "        first_forecast_price = forecasts[0]\n",
    "        \n",
    "        # Plot the connecting line (bridge)\n",
    "        plt.plot([last_historical_date, first_forecast_date], \n",
    "                [last_historical_price, first_forecast_price], \n",
    "                color='orange', linewidth=2, linestyle='-', alpha=0.8, \n",
    "                label='Historical-Forecast Bridge')\n",
    "        \n",
    "        # Plot forecasts (connected line)\n",
    "        extended_forecast_dates = [last_historical_date] + list(future_dates)\n",
    "        extended_forecasts = [last_historical_price] + list(forecasts)\n",
    "        \n",
    "        plt.plot(extended_forecast_dates, extended_forecasts, \n",
    "                label='Forecasted Prices', linewidth=2, color='red', \n",
    "                marker='s', markersize=5, linestyle='--', alpha=0.9)\n",
    "        \n",
    "        # Add a vertical line to separate historical and forecasted data\n",
    "        plt.axvline(x=last_historical_date, color='blue', linestyle=':', alpha=0.5, linewidth=1, \n",
    "                   label='Forecast Start')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f'Cinnamon Price Forecast: {grade.title()} Grade in {region.title()}\\n'\n",
    "                 f'Historical Data vs {months_ahead}-Month Forecast', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Regional Price', fontsize=12)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis dates\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add confidence bands (simplified approach using standard deviation)\n",
    "        if len(forecasts) > 1:\n",
    "            forecast_std = np.std(subset['Regional_Price'].tail(12))  # Use last 12 months std\n",
    "            upper_bound = np.array(extended_forecasts[1:]) + 1.96 * forecast_std  # Exclude bridge point\n",
    "            lower_bound = np.array(extended_forecasts[1:]) - 1.96 * forecast_std\n",
    "            \n",
    "            plt.fill_between(future_dates, lower_bound, upper_bound, \n",
    "                           alpha=0.2, color='red', label='95% Confidence Interval')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print forecast summary\n",
    "        print(f\"\\n{grade.title()} Grade Forecast for {region.title()}:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, (date, price) in enumerate(zip(future_dates, forecasts), 1):\n",
    "            print(f\"Month {i:2d} ({date.strftime('%Y-%m')}): LKR.{price:8.2f}\")\n",
    "        \n",
    "        print(f\"\\nForecast Statistics:\")\n",
    "        print(f\"Average Forecast Price: LKR.{np.mean(forecasts):.2f}\")\n",
    "        print(f\"Price Range: LKR.{np.min(forecasts):.2f} - LKR.{np.max(forecasts):.2f}\")\n",
    "        \n",
    "        # Calculate trend\n",
    "        if len(forecasts) > 1:\n",
    "            trend = (forecasts[-1] - forecasts[0]) / len(forecasts)\n",
    "            trend_direction = \"increasing\" if trend > 0 else \"decreasing\"\n",
    "            print(f\"Overall Trend: {trend_direction} by LKR.{abs(trend):.2f} per month\")\n",
    "        \n",
    "        return forecasts, future_dates\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting forecast results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "available_grades = df['Grade'].unique()\n",
    "available_regions = df['Region'].unique()\n",
    "\n",
    "print(f\"Available grades: {available_grades}\")\n",
    "print(f\"Available regions: {available_regions}\")\n",
    "\n",
    "# Generate forecast for first combination\n",
    "if len(available_grades) > 0 and len(available_regions) > 0:\n",
    "    grade_to_forecast = available_grades[0]\n",
    "    region_to_forecast = available_regions[0]\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Generating forecast for {grade_to_forecast} in {region_to_forecast}...\")\n",
    "    forecasts1, future_dates1 = plot_forecast_results(\n",
    "        df, model, grade_to_forecast, region_to_forecast, months_ahead=6\n",
    "    )\n",
    "\n",
    "# Try another combination if available\n",
    "if len(available_grades) > 1 and len(available_regions) > 1:\n",
    "    grade_to_forecast2 = available_grades[1] if len(available_grades) > 1 else available_grades[0]\n",
    "    region_to_forecast2 = available_regions[1] if len(available_regions) > 1 else available_regions[0]\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Generating forecast for {grade_to_forecast2} in {region_to_forecast2}...\")\n",
    "    forecasts2, future_dates2 = plot_forecast_results(\n",
    "        df, model, grade_to_forecast2, region_to_forecast2, months_ahead=6\n",
    "    )\n",
    "\n",
    "def interactive_forecast():\n",
    "    \"\"\"Allow user to select grade and region for forecasting\"\"\"\n",
    "    print(\"\\nðŸ”® Interactive Forecast Generation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display available options\n",
    "    print(\"\\nAvailable Grades:\")\n",
    "    for i, grade in enumerate(available_grades, 1):\n",
    "        print(f\"{i}. {grade}\")\n",
    "    \n",
    "    print(\"\\nAvailable Regions:\")\n",
    "    for i, region in enumerate(available_regions, 1):\n",
    "        print(f\"{i}. {region}\")\n",
    "    \n",
    "    # Get user input\n",
    "    try:\n",
    "        grade_idx = int(input(\"\\nSelect grade number: \")) - 1\n",
    "        region_idx = int(input(\"Select region number: \")) - 1\n",
    "        months = int(input(\"How many months to forecast (default 6): \") or \"6\")\n",
    "        \n",
    "        if 0 <= grade_idx < len(available_grades) and 0 <= region_idx < len(available_regions):\n",
    "            selected_grade = available_grades[grade_idx]\n",
    "            selected_region = available_regions[region_idx]\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Generating {months}-month forecast for {selected_grade} in {selected_region}...\")\n",
    "            forecasts, dates = plot_forecast_results(\n",
    "                df, model, selected_grade, selected_region, months_ahead=months\n",
    "            )\n",
    "            return forecasts, dates\n",
    "        else:\n",
    "            print(\"Invalid selection. Please try again.\")\n",
    "            return None, None\n",
    "            \n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Error: {e}. Please enter valid numbers.\")\n",
    "        return None, None\n",
    "\n",
    "interactive_forecast()\n",
    "\n",
    "# ============ HYPERPARAMETER TUNING SUMMARY ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if metrics.get('tuning_used'):\n",
    "    print(f\"âœ… Hyperparameter tuning was performed using: {metrics['tuning_method']}\")\n",
    "    print(f\"ðŸ† Best parameters found:\")\n",
    "    for key, value in metrics['best_params'].items():\n",
    "        print(f\"   â€¢ {key}: {value}\")\n",
    "    print(f\"\\nðŸ“Š Final Model Performance:\")\n",
    "    print(f\"   â€¢ MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"   â€¢ RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"   â€¢ RÂ²: {metrics['r2']:.4f}\")\n",
    "    print(f\"   â€¢ Training epochs: {metrics['epochs_trained']}\")\n",
    "else:\n",
    "    print(\"âŒ Hyperparameter tuning was skipped - default parameters used\")\n",
    "\n",
    "print(\"\\nðŸ’¡ To use different tuning methods, modify these variables:\")\n",
    "print(\"   â€¢ USE_HYPERPARAMETER_TUNING = True/False\")\n",
    "print(\"   â€¢ TUNING_METHOD = 'optuna'/'grid'/'random'\")  \n",
    "print(\"   â€¢ N_TUNING_TRIALS = number of trials\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
